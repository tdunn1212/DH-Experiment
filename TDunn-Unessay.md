# Build your own girlfriend! (*reciprocity not included*)
> In this article, I break down my DH experiment, outlining my goals, process, and larger implications it has for my thesis project.

By Theo Dunn, DIGH 5000

## Introduction
The mass-implementation and advertisement of generative artificial intelligence (AI) has made it so that AI resembles a kind of black box, so opaque that it becomes challenging for even seasoned researchers to understand the processes that enable algorithmic decision-making (ADM). This is prolifically worsened by the amount of data and programming that contributes to the inputs/outputs of large language models (LLMs) like ChatGPT. Modern LLMs are trained on datasets so large that it is generally impossible for human minds to ever see, never mind process, this data. ChatGPT 4.0 is trained on 1 petabyte (PB) worth of data, or 1,000 TB. This is equivalent to "180 [United States] Libraries of Congress; 500 billion pages of standard printed text" ([as of 2011](https://intellobics.com/2011/02/14/understanding-information-measures/)). 

The lack of clear accountability and explanation we are able to attribute to AI and algorithms makes their decision-making and output process uniquely opaque. This requires intense trepidation and scrutiny in where and how we implement ADM and AI in informal roles. Part of this scrutiny involves experimentation and study replicating conditions LLMs are trained under to distill ADM and demonstrate why its opacity far surpasses human decision-making ([Peters, 2022](https://link.springer.com/article/10.1007/s43681-022-00217-w)). Of course, this is extremely pressing in settings where ADM is used like medical, economic, and law enforcement initiatives. However, the influences of algorithmic training decisions will be just as relevant in low-stakes social settings, where AI has the potential to shape social and cultural beliefs.

My thesis work involves examining patriarchal harms enabled by LLMs used as "AI Girlfriends". I am arguing that AI Girlfriends are responsible for perpetuating algorithmic bias which, if left unchecked, will diminish the autonomy, consent, and personhood of women. Beyond gendered harms, AI as romantic partners could also deteriorate the emotional vulnerability, growth, and challenges that make close emotional relationships so crucial for development. In my research, I will be performing philosophical feminist analysis of AI relationships, but as a trained multimedia researcher, I have also been adamant in the importance of public-facing and empirical components to this work. In this first preliminary attempt to working with AI to analyze algorithmic biases and harms present in AI relationships, I sought to answer the question: *can using data from scripts and other media sources distill patriarchal influences that might help us glean how AI chatbots are trained to resemble "girlfriends"?*  I will distill my process in answering this question by first giving an overview of my research, highlighting my process step-by-step, illustrating my conclusions, and lastly, outlining my next steps and key takeaways. 

## My Research and Goals
As previously mentioned, for the past 6 months, I've been in the business of critiquing AI girlfriends as both perpetuating and being symptomatic of harms to our social systems. This research has the ultimate goal of encouraging feminist relational challenges to our social systems; widespread social isolation combined with patriarchal factors that dissuade people from accepting vulnerability in social settings has created the space for AI girlfriends to be so successful. If people felt that they could comfortably be in community with one another, and that— especially for men— vulnerability and rejection were not signs of weakness, people would likely not feel that it is necessary to supplement human-to-human connection with AI. To illustrate this point, I will survey a couple of key sources/concepts which demonstrate my motivations to carry out this experiment. 
### Survey
#### Relational Artefacts, Sherry Turkle
MIT Sociologist/Psychologist Sherry Turkle has been concerned with people's relationships with AI since [ELIZA](https://eliza.dotink.co/). In the 80s and 90s, she turned her focus to Furbies and Tamagotchis, but now she is concerned with the advent of AI as romantic partners and supplements for human-to-human relationships. 

Turkle has invented a useful term to define all of these AI/robots under one umbrella: relational artefacts. Relational artefacts are *“computational objects[s] explicitly designed to engage a user in a relationship”* ([Turkle, 2007, pg. 502](https://www.jbe-platform.com/content/journals/10.1075/is.8.3.11tur)). Using the terminology of relational artefacts, Turkle argues that the lack of reciprocity in AI prevents it from being an authentic social connection. This lack of authenticity and LLMs mimicry of our social attitudes and desires will diminish our need for togetherness and social connection. AI relationships create the illusion of meeting our social needs, but in actuality, they are like a mirage, or what Turkle sees as "offering a photograph of water to quench thirst" ([Turkle and Pataranutaporn, 2024](https://www.wsj.com/tech/ai/a-14-year-old-boy-killed-himself-to-get-closer-to-a-chatbot-he-thought-they-were-in-love-691e9e96)). 
#### The Right to Sex, Amia Srinivasan
In her essay, *The Right to Sex,* Amia Srinivasan argues that *there is no right to sex, but that people's "fuckability" is socially-mediated by factors like racism, patriarchy, ableism, etc.* (Srinivasan, 2021, p. 90). In her analysis, Srinivasan argues that some people have disproportionately less access to sex, due to them not fitting oppressive beauty standards. However, conversations around the inequity of sexual desire has been coopted by men identifying as incels (a portmanteau for involuntary celibate), who blame women for their lack of sexual access, treating them as a less-evolved subclass of human.

As part of my feminist analysis of AI relationships, I focus on AI girlfriends due to their use as part of a presumed right to sex. AI girlfriends are advertised to the kinds of men who cannot find sexual connection with women and blame women for this lack of connection. An AI girlfriend advertises the benefits of sexual intimacy without the drawback of having to deal with a kind of human they see as superficial and irrational. Not only that, but AI girlfriends replace men/incels' needs to subject themselves to what they see as the dating game, where they—as a lower caste of men— are largely unable to find women to sleep with them. AI girlfriends will function as perpetuations of this kind of belief.
#### Misogyny, Kate Manne
Briefly, philosopher Kate Manne uses analytic philosophy to define and specify the concept of misogyny to better combat it. She wants to resist the presumption that misogyny is simply a psychological or epistemological defect of an agent. Manne defines misogyny as:
> *“a property of social environments in which women are liable to encounter hostility due to the enforcement and policing of patriarchal norms and expectations—often, though not exclusively, insofar as they violate patriarchal law and order. Misogyny hence functions to enforce and police women’s subordination and to uphold male dominance, against the backdrop of other intersecting systems of oppression and vulnerability”* (Manne, 2019, p.19)

### Research Experiment Goals & Thesis Significance
As previously mentioned, I want my work to implement accessible and multimedia methodology to better reach a wider audience and promote relational results. DH heavily aligns with these goals. My research in this experiment and subsequent empirical thesis work seeks to specifically distill algorithmic bias in AI partners, and emphasize illusory aspects of AI relationships. Given the opaqueness of ADM, it is greatly beneficial to specifically distill and illustrate how AI can perpetuate harm. This is also relevant given how current AI adoption appeals to mitigating challenges and possibilities of failure. Evidence-based work helps combat cognitive dissonance that AI-based harms are exaggerated.
## Experimentation Process
In this section, I will outline the main steps and iterations of my experiment, reflecting on and listing my main steps to trace the development of my research and promote transparency. Each of the headings for my main steps will contain a hyperlink to my lab notes matching the corresponding section. 
### [One](DH-Experiment-1.md)
In this step, I first outlined my research question and what steps I needed to take to achieve that. Taking down the main components of the experiment requirements, I proceeded to brainstorm my question before arriving at my main research question:

>*Can using data from scripts and other media sources distill patriarchal influences that might help us glean how AI chatbots are trained to resemble "girlfriends"?*

With this, I broke down my research idea into main steps so that I could space out the workload and ensure I did it so that it would be easily understandable and replicable by any readers. I forgot to note this down, but it was in this step that I also experimented with [Shawn Graham's GitHub notebooks](https://github.com/shawngraham/pn_notebooks) on breaking down AI technology to help make their processes and shortcomings more transparent. I used his first notebook, titled "Practical Necromancy," as code that would help me apply my very beginner Python skills to train a LLM with minimal stress. 

I first ran Dr. Graham's code, trying my best to see how it works and what I could use from it to apply to my own experiment. Once I did this and experimented with the different LLM outputs, I decided I would next try making my own iteration in my first attempt to make an LLM that might rudimentarily resemble an AI GF.
### [Two](DH-Experiment-2.md)
In my first attempt, I used the dialogue files from *Stardew Valley* (SDV) to try and create a LLM that would mimic the dialogue of characters specifically geared to have conversations that make them appear attractive to players. I decided to use Haley, since much of her character is hyperfeminine and superficial until her development in later cutscenes.

However, these files are in JSON format, which made the parsing and formatting slightly challenging. I decided to proceed even with inaccuracies, since I was more concerned with getting the preliminary coding correct. For the most part, this experiment was mostly just to make sure I could modify the code and make it suit my purposes in my experiment. This did work, however, the formatting of the JSON file and the small amount of data made the results predictably poor-quality (see below).
![](/images/4-07-02)
These results are not great, but they show that I can modify the code and train it to create conversational dialogue. In this training, I also reflected on my goals and determined that especially with my limited coding experience, it is better to gear my efforts towards developing my programming experience to successfully create my replication of an AI girlfriend large language model (LLM), to distill the sophistication, obscurification, and potential harms in AI GFs. It helped me to determine the steps I needed to take for the rest of the experiment.
### [Three](DH-Experiment-3.md)
The results of this experiment were similar to the initial results of my last coding session, except I changed the file to a txt file and removed some of the JSON language. I decided that since it would be more time-consuming than beneficial for me to continue converting JSON to txt, I left the SDV dialogue behind to focus instead on movie scripts instead of video game files.

However, I did use the txt file I converted to try and train GPT2 again, this time with Abigail instead of Haley. I also slightly modified the training and output variables to make the results shorter and more geared to my goals. So these results were beneficial, but not ultimately the trajectory the rest of my experiment would be taking.
![](/images/4-08-03)
### [Four](DH-Experiment-4.md)
In this stage, I moved my code to working locally in VSCode, which ultimately helped for programming, but made running the code much harder, due to the GPU requirements for training LLMs. In this stage, I also started to work with the *Her* script, as scripts became much easier to work through compared to video game dialogue, due to their formatting and their larger datasets. I wrote a function that would take only Samantha's (the AI love interest) dialogue and put it in a separate text file. The output looked like this:
![](/images/4-08-04)
One challenge in working with txt files is that many special characters did not translate to the txt format, so there are some broken strands of text. I would have to do more future troubleshooting to accommodate this. I then adapted the code from Practical Necromancy to train and output data based on the *Her* script. The final results took the following form:
![](/images/4-08-06)
These results were definitely on the right track towards what I want to produce in my final experiment. This specific stage of my project was very fruitful in building my confidence and plans for further coding work. It also helped me determine my next steps of both adding more dialogue to the training data and performing more specified tests with enhanced training.
### [Five](DH-Experiment-5.md)
This is where I feel like I hit the ground running with my coding process. I decided to add script data from Ava in *Ex Machina*, going with the robot love interest theme. 
![](/images/4-09-01)
Given how easy this was, I wanted to do it ten more times at once, but I knew it was best to take smaller steps and work my way up from there. I then concatenated these two files to create my dataset to train GPT2, with 270~ lines of dialogue.
![](/images/4-09-02)
This, combined with the training stage, brought me to my final results for this stage (below). It seems like the model preferred to use Samantha's dialogue more, which makes sense given that Samantha has more dialogue to work with and her tone is warmer overall. These results were still promising and helped me to feel more confident in my programming skills. 
![](/images/4-09-03)
### [Six](DH-Experiment-6.md)
Following my success in the last stage of my experimentation, I decided to add several more characters to the dataset to expand the variability in outputs that my model generates. Using the scripts from *Scott Pilgrim*, *The Matrix*, and *Uncharted 2*, I added three characters from *Scott Pilgrim* (Ramona, Knives, and Envy, all love interests), Trinity from *the Matrix*, and Chloe from *Uncharted 2*. Using the same format of redeclaring variables and running my function, I extracted all of these characters and then trained GPT2 on all of their data. The only thing I had to change in this process was writing a slightly modified function to extract Chloe's dialogue, since the Uncharted script used comma separators instead of linebreaks. This also greatly expanded my dataset to roughly 850 lines of dialogue, making the training data much larger and creating more potential for variable results.
![](/images/4-09-05)
In my results, I found that the model still prioritized Samantha's dialogue to a certain extent, but answers started to become a lot more varied. I decided I would import my code back into Colab for the next steps, so that I can train it in more detail and speed. I also considered if there was any way to ensure that the model could ensure that it is only writing contained sentences, but I was unable to explore this within the scope of this experiment. 
### [Seven](DH-Experiment-7.md)
Not much happened in this experiment session, I simply wanted to try my expanded training data in Colab with more training steps to see if it enhanced the coherence of my results. The results were slightly better, but mot overwhelmingly so:
![](/images/4-09-06)
I realized that in this stage, it wouldn't be easy to make sure sure that my results are fully coherent, as this would involve making my LLM much more sophisticated and going beyond GPT2's capabilities. I decided that I would add more data to my training catalogue and work with more sophisticated tests.
### [Eight](DH-Experiment-8.md)
For expanding the training data, I ended up taking a step backwards, which didn't end up being too consequential. I added dialogue from Tifa and Aerith from *Final Fantasy VII*, which is a massive game and a stark departure from the rest of my training data. As a result, Aerith and Tifa both have a ton of lines, which skewed a lot of the model's outputs. Thankfully, this didn't end up being too time consuming, since I had only redefined the function I used to extract Chloe's dialogue.
![](/images/4-09-09)
I knew I had to go in a different direction when GPT2 called me Sephiroth... I decided in my end reflection of that experimentation session that I would try out some rom-com scripts instead.
### [Nine](DH-Experiment-9.md)
As I decided in my previous training session, I ended up adding the data from *500 Days of Summer* and *You've Got Mail* to get dialogue that is both strictly romantic and grounded in reality rather than fantasy. In my last training session, I had also discovered that I reached the daily limit of Colab's free T4 GPU use, so I had to do less training in VSCode until the following day. However, I felt like after the step back I took last time I trained the model, this was a step forward, as the results were (mostly) very promising. 
![](/images/4-10-01)
![](/images/4-10-02)
Many of these outputs still have the same incoherencies that have started to frustrate me, but I've accepted that to remedy this, I would have to go into much more detail and train potentially a more advanced LLM to resolve this issues. However, at this stage, I felt much closer to getting to a complete version of my code. I knew that once I could return to Colab, a more rigorously trained version of my model would likely get me the results I want for this stage.
### [Ten](DH-Experiment-10.md)
In this step, I carried out my final experiment, which I won't go over in extreme detail, since the actual notes are roughly as long as this essay. However, I encourage you to go through the page and see the model's different results. Without jumping the gun, I overall found that there were still many parts of my outputs that I was unsatisfied with, but compared to where I started, I'm proud of the visible improvements I've made and there is great potential for future experiments in this area.

For the parameters of this test, I first trained my model with the most steps yet (2000) and then decided to try a number of different prompts while modifying the randomness values of the model. 

Overall, I found that I got the kinds of results that might imply illusive reciprocal connections, but it is hard to effectively gauge the efficacy of AI GF models without a conversational output and much more data. I was glad to be able to have this first step done and it makes complexifying and presenting code as part of my thesis work feel much more feasible.
###### Sample View of Results below:
```
You deserve to be happy! Youre the world's face! Keep walking. Okay, now you've lost me... 
==================== 
You deserve to be happy! Youre the crazy person! One day Im reading a book at the corner deli and 
==================== 
You deserve to be happy. You and me. It would be better if we were just like you. Just like you 
==================== 
You deserve to be happy. Youre a successful guy - youre a successful woman - so dont back out on me. 
==================== 
You deserve to be happy. Youre a successful person. Well, youre not happy here. Well, you 
====================
```
## Results and Reflections
To return to my initial question- *can using data from scripts and other media sources distill patriarchal influences that might help us glean how AI chatbots are trained to resemble "girlfriends"?* **Kind of...?** My initial results are helpful insofar as they do emulate existing patriarchal biases present in popular media, and that AI models whose goals are to "objectively" replicate these texts will inevitably fall victim to these biases. I will stress that I am a researcher with specific beliefs and goals, so choosing the films/games I chose may reflect the proof I am trying to argue for.

Ultimately, this experiment is most helpful as a way to infer what kind of biases and latent messages are present in complex LLMs. If a very rudimentary LLM with particular high-quality information can easily fall victim to this information, we can easily predict that LLMs trained on all this data and more will also contain these biases.

I know that if I want this work to provide a fruitful angle for my thesis, I will have to continue with it, working with more advanced models and working in a chatbot format, which helps to facilitate illusions of reciprocity. 

I also think that it's important to stress that given the small and curated selection of data I use to train GPT2, we need to be even more skeptical of commercial LLMs like ChatGPT 4.o, since these models scrape the web on a mass scale, no doubt picking up on many harmful discriminatory texts.

So long story short, we can definitely learn something from this avenue of experimentation, but I don't think that this work is anything to write home about yet. AI is definitely capable of relational deception and hasty affirmation, but working in chatbot models will help illustrate this point. One future challenge for my research is balancing being able to train models based on significantly more data while still ensuring that this data is transparent and that my own research doesn't become a black box.
## Next Steps
My next steps include finding how to work in a chatbot model that I can easily train while still maintaining an understanding of the data that the LLM is drawing upon. Creating a more varied and sophisticated LLM helps to guarantee the validity of my research, formatting my work in a way that simultaneously uses theoretical and empirical evidence to stress the urgent harms of relational artefacts. 

I also think it would be valuable to do investigative work on the kinds of data used to train AI girlfriends, which companies treat as their "secret formula," making this information hard to find. 
![A Spongebob screenshot of Mr. Krabbs and Plankton fighting over the Krabby Patty secret formula](https://static1.srcdn.com/wordpress/wp-content/uploads/2020/03/SpongeBob-SquarePants-Mr-Krabs-secret-formula-Plankton.jpg)
*(Pictured above: AI companies fighting over training data that isn't their IP in the first place)*
However, I have good reason to suspect that a lot of this training data depicts a one-dimensional, misogynistic view of women, which is what makes AI GFs so satisfying to their users. Doing investigative work by infiltrating private forums of people with AI GFs helps me find out what this data contains. I have already tried many times to reach out to many different companies, but haven't yet gotten any responses.
## Key TL;DR Takeaways
Here are some main takeaways and next steps for my work, summarized into bullet-points.
1. My findings are not enough to definitively argue that AI GF LLMs contain and perpetuate patriarchal ideology, but we can definitely continue to infer that this is the case by training and examining a very basic and small LLM.
2. If we can identify misogynistic bias in a very basic LLM, it is likely that sophisticated ones share this data and more.
3. By continuing to do this work carefully, to not risk accidentally making a "black box AI", we can implement chatbot LLMs to represent both gendered algorithmic discrimination, and how chatbot LLM models perpetuate illusive connections with their human users.

Thanks for reading!
## References
- Boba. (2020, April 22). _Understanding information measures - Intellobics_. Intellobics. https://intellobics.com/2011/02/14/understanding-information-measures/
- Castelvecchi, D. (2016). THE BLACK BOX. In _NATURE: Vol. VOL 538_ (pp. 21–21) [FEATURE]. Macmillan Publishers Limited. https://www.nature.com/news/polopoly_fs/1.20731!/menu/main/topColumns/topLeftColumn/pdf/538020a.pdf
- Laizure, S. C. (2024). Caution: ChatGPT doesn’t know what you are asking and doesn’t know what it is saying. _The Journal of Pediatric Pharmacology and Therapeutics_, _29_(5), 558–560. https://doi.org/10.5863/1551-6776-29.5.558
- Manne, K. (2019). _Down girl: The Logic of Misogyny_. Oxford University Press, USA.
- Peters, U. (2022). Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque. _AI And Ethics_, _3_(3), 963–974. https://doi.org/10.1007/s43681-022-00217-w
- Srinivasan, A. (2021). _The right to sex: Feminism in the Twenty-First Century_. Farrar, Straus & Giroux.
- Turkle, S. (2007). Authenticity in the age of digital companions. _Interaction Studies Social Behaviour and Communication in Biological and Artificial Systems_, _8_(3), 501–517. https://doi.org/10.1075/is.8.3.11tur
- Turkle, S., & Pataranutaporn, P. (2024, November 8). _A 14-Year-Old Boy Killed Himself to Get Closer to a Chatbot. He Thought They Were In Love._ The Wall Street Journal. https://www.wsj.com/tech/ai/a-14-year-old-boy-killed-himself-to-get-closer-to-a-chatbot-he-thought-they-were-in-love-691e9e96
